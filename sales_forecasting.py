# -*- coding: utf-8 -*-
"""Sales Forecasting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LDt40HuskEKNrmwDlbXoqjygbSfrOTKf

# Sales Prediction

## Objective

The data scientists have collected sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store (each row of data).

__So the idea is to find out the features (properties) of a product, and store which impacts the sales of a product.__

## Dataset Details

![](https://i.imgur.com/WlgNuFs.png)
"""

import pandas as pd
df = pd.read_csv('sales_prediction.csv')

df.head()

"""# Preparing Training and Test Datasets
## 70% Train 30% Test
## seed = 42 for reproducibility
"""

X = df.drop(columns=['Item_Outlet_Sales'])
y = df['Item_Outlet_Sales']

SEED = 42

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = SEED)
X_train.shape, X_test.shape,y_test.shape

X_train.head()

y_train.head()

"""# Data Wranging + EDA + Feature Engineering"""

X_train_c = X_train.copy()

"""## Basic EDA"""

X_train_c.info()

X_train_c.isnull().sum()

num_data = X_train_c.select_dtypes(exclude=['object'])
num_data.head()

num_data.describe()   ## to check outliers or any type of negative or illogical values

num_data.isnull().sum()

import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots(1,2, figsize = (12,5))

sns.histplot(data=X_train_c,x='Item_Weight',ax=ax[0]);
sns.boxplot(data=X_train_c,y='Item_Weight',ax=ax[1]);

#### remove this in the final copy

def visualize_numeric_feature(data_frame, col_name):
  fig, ax = plt.subplots(1,2,figsize=(12,5))
  sns.histplot(data=data_frame,x=col_name,ax=ax[0]);
  sns.boxplot(data=data_frame,y=col_name,ax=ax[1]);

visualize_numeric_feature(X_train_c, 'Item_Weight')

visualize_numeric_feature(X_train_c, 'Item_Visibility')

visualize_numeric_feature(X_train_c, 'Item_MRP')

visualize_numeric_feature(X_train_c, 'Outlet_Establishment_Year')  #Results are not useful as Year is a discrete term and thus countplot might give us a better insight

sns.countplot(data = X_train_c,x='Outlet_Establishment_Year')  ##Tells us how many products were sold in a specific year

cat_features = X_train_c.select_dtypes(include=['object'])
cat_features.head()

cat_features.describe()

cat_features.isnull().sum()

cat_features['Item_Identifier'].value_counts()

cat_features['Item_Fat_Content'].value_counts() ## We need to standardize the low fat and regular terms

cat_features['Item_Type'].value_counts()

cat_features['Outlet_Identifier'].value_counts()

cat_features['Outlet_Size'].value_counts()

cat_features['Outlet_Location_Type'].value_counts()

cat_features['Outlet_Type'].value_counts()

"""## Data Wrangling and Feature Engineering"""

#1 Item identifier (creating high level ids)

X_train_c['Item_Identifier'].apply(lambda x: x[:2])

X_train_c['Item_Identifier'].apply(lambda x: x[:2]).value_counts()
X_train_c['Item_Identifier'].str[:2].value_counts()  ##Same thing

def create_item_type(data_frame):
  data_frame['Item_Type'] = data_frame['Item_Identifier'].str[:2]   ## Extract 1st two characters
  data_frame['Item_Type'] = data_frame['Item_Type'].map({           ## Assigning to new column and renaming the columns
      'FD':'Food',
      'NC':'Non_Consumables',
      'DR':'Drink'
  })
  return data_frame

X_train_c = create_item_type(X_train_c)
X_train_c.head()

#2 Handling missing values in Item_Weight

X_train_c.isnull().sum()

X_train_c[['Item_Identifier','Item_Weight']].drop_duplicates().sort_values(by=['Item_Identifier'])

# Mapping based on existing data
# If failed then filling the missing data with the median of the corresponding category (Reason for failure might be completely different id in the training set)

X_train_c[['Item_Type','Item_Weight']].drop_duplicates().sort_values(by=['Item_Type'])

#  Mapping based on existing data

ITEM_ID_WEIGHT_PIVOT = X_train_c.pivot_table(values='Item_Weight', index='Item_Identifier').reset_index()
ITEM_ID_WEIGHT_MAPPING = dict(zip(ITEM_ID_WEIGHT_PIVOT['Item_Identifier'],ITEM_ID_WEIGHT_PIVOT['Item_Weight']))
list(ITEM_ID_WEIGHT_MAPPING.items())[:10]

# In case of Failure
ITEM_TYPE_WEIGHT_PIVOT = X_train_c.pivot_table(values='Item_Weight',index='Item_Type',
                                        aggfunc='median').reset_index()
ITEM_TYPE_WEIGHT_MAPPING = dict(zip(ITEM_TYPE_WEIGHT_PIVOT['Item_Type'],ITEM_TYPE_WEIGHT_PIVOT['Item_Weight']))
ITEM_TYPE_WEIGHT_MAPPING.items()

def impute_item_weight(data_frame):
  #1st Logic
  data_frame.loc[:,'Item_Weight'] = data_frame.loc[:,'Item_Weight'].fillna(data_frame.loc[:,'Item_Identifier'].map(ITEM_ID_WEIGHT_MAPPING))
  #2nd Logic
  data_frame.loc[:,'Item_Weight'] = data_frame.loc[:,'Item_Weight'].fillna(data_frame.loc[:,'Item_Type'].map(ITEM_TYPE_WEIGHT_MAPPING))
  return data_frame

X_train_c = impute_item_weight(X_train_c)

X_train_c.isnull().sum()

#3 Handling missing values in Outlet_Size

X_train_c.groupby(by=['Outlet_Type','Outlet_Size']).size()

# if grocery store then small
# if Supermarket Type1 then small -- most frequent -- small
# if Supermarket Type2 then Medium
# if Supermarket Type3 then Medium



#54

X_train_c.isnull().sum()

X_train_c = X_train_c.drop('Outlet_Size', axis=1)

X_train_c.head()

X_train_c.isnull().sum()

# 4 Standardize Item_Fat_Content categories

X_train_c['Item_Fat_Content'].value_counts()

def standardize_item_fat_content(data_frame):
  data_frame['Item_Fat_Content'] = data_frame['Item_Fat_Content'].replace({
      'Low Fat' :'Low_Fat',
      'LF' :'Low_Fat',
      'low fat' :'Low_Fat',
      'reg' :'Regular'


  })
  return data_frame

X_train_c = standardize_item_fat_content(X_train_c)
X_train_c['Item_Fat_Content'].value_counts()

#5 Correct Item fat content for non-consumables

X_train_c.groupby(by=['Item_Type','Item_Fat_Content']).size()   ##Non-consumables cant have fat content

X_train_c.loc[X_train_c['Item_Type'] == 'Non_Consumables','Item_Fat_Content']

def correct_item_fat_content(data_frame):
  data_frame.loc[data_frame['Item_Type'] == 'Non_Consumables','Item_Fat_Content'] = 'Non_Edible'
  return data_frame

X_train_c  = correct_item_fat_content(X_train_c)
X_train_c.groupby(by=['Item_Type','Item_Fat_Content']).size()

X_train_c.info()

"""# Preparing Datasets for ML

"""

def prepare_dataset(data_frame):
  #Step 1:Create Item Type
  data_frame = create_item_type(data_frame)
  #Step 2:Fill in missing values for Item_Weight
  data_frame = impute_item_weight(data_frame)
  #Step 3: Outlet Size
  #Step 4: Make item fat content categories consistent
  data_frame = standardize_item_fat_content(data_frame)
  #Step 5: Correct item fat content for non-consumables
  data_frame = correct_item_fat_content(data_frame)

  return data_frame

X_train.isnull().sum()

X_train = prepare_dataset(X_train)
X_train = X_train.drop('Outlet_Size', axis=1)
X_train.isnull().sum()

X_test.isnull().sum()

X_test = prepare_dataset(X_test)
X_test = X_test.drop('Outlet_Size', axis=1)
X_test.isnull().sum()

"""# Handling Categorical Data

## Expt1 : All categorical columns - one hot encoded
"""

cat_feats = X_train.select_dtypes(include=['object'])
cat_feats.head()

from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(handle_unknown='ignore')
ohe.fit(cat_feats)

ohe_feature_names=ohe.get_feature_names_out(input_features=cat_feats.columns)
ohe_feature_names

num_feats_train = X_train.select_dtypes(exclude=['object']).reset_index(drop=True)
num_feats_train.head()

cat_feats_train = X_train.select_dtypes(include=['object'])
X_train_cat_ohe = pd.DataFrame(ohe.transform(cat_feats_train).toarray(),columns=ohe_feature_names)
X_train_cat_ohe.head()

X_train_final = pd.concat([num_feats_train, X_train_cat_ohe], axis=1)
X_train_final.head()

final_columns = X_train_final.columns.values
final_columns

num_feats_test = X_test.select_dtypes(exclude=['object']).reset_index(drop=True)
cat_feats_test = X_test.select_dtypes(include=['object'])
X_test_cat_ohe = pd.DataFrame(ohe.transform(cat_feats_test).toarray(),columns=ohe_feature_names)
X_test_final = pd.concat([num_feats_test, X_test_cat_ohe], axis=1)
X_test_final = X_test_final[final_columns]

X_test_final.head()

"""### Modeling"""

sns.histplot(y_train)

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor
import xgboost as xgb
from lightgbm import LGBMRegressor
from sklearn.model_selection import cross_validate
import numpy as np

def train_and_eval_model(model, X_train, y_train,cv=5):
  cv_results = cross_validate(model,X_train,y_train,cv=cv,scoring=('r2','neg_root_mean_squared_error'),)
  print('Model:' ,model)
  r2_scores = cv_results['test_r2']
  print('R2 CV scores:',r2_scores)
  print('R2 CV scores mean/stdev:',np.mean(r2_scores),'/',np.std(r2_scores))

  rmse_scores = cv_results['test_neg_root_mean_squared_error']
  rmse_scores = [-1*score for score in rmse_scores]
  print('RMSE CV scores:',rmse_scores)
  print('RMSE CV scores mean/stdev:',np.mean(rmse_scores),'/',np.std(rmse_scores))

rf = RandomForestRegressor(random_state=SEED)
train_and_eval_model(model=rf, X_train=X_train_final, y_train=y_train)

gb = GradientBoostingRegressor(random_state=SEED)
train_and_eval_model(model=gb, X_train=X_train_final, y_train=y_train)

hgb = HistGradientBoostingRegressor(random_state=SEED)
train_and_eval_model(model=hgb, X_train=X_train_final, y_train=y_train)

xgr = xgb.XGBRegressor(objective='reg:squarederror',random_state=SEED)
train_and_eval_model(model=xgr, X_train=X_train_final, y_train=y_train)

lgbr = LGBMRegressor(random_state=SEED)
train_and_eval_model(model=lgbr, X_train=X_train_final, y_train=y_train)

"""## Expt2: All categorical columns: Native Handling"""

X_train_copy = X_train.copy().drop(columns = 'Item_Identifier')

cat_cols = X_train_copy.select_dtypes(include=['object']).columns.tolist()
num_cols = cal_cols = X_train_copy.select_dtypes(exclude=['object']).columns.tolist()

cat_cols,num_cols

#1:12

X_train_copy[cat_cols] = X_train_copy[cat_cols].astype('category')
n_categorical_features=len(cat_cols)
n_numerical_features=len(num_cols)
X_train_copy = X_train_copy[cat_cols+num_cols]

X_train_copy.info()

from sklearn.preprocessing import OrdinalEncoder
from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer
from sklearn.compose import make_column_selector

categorical_mask = [True]*n_categorical_features+[False]*n_numerical_features

ordinal_encoder = make_column_transformer(
    (
        OrdinalEncoder(handle_unknown="use_encoded_value",unknown_value=np.nan),
        make_column_selector(dtype_include="category"),
    ),
    remainder = "passthrough",
)

hgb = make_pipeline(
    ordinal_encoder,
    HistGradientBoostingRegressor(
        random_state=SEED,categorical_features=categorical_mask
    ),
)

train_and_eval_model(model=hgb, X_train=X_train_copy, y_train=y_train)

lgbr = LGBMRegressor(random_state=SEED)
train_and_eval_model(model=lgbr, X_train=X_train_copy, y_train=y_train)

"""## Expt3 : No Item Identifier - one hot encoded"""

cat_feats = X_train.select_dtypes(include=['object']).drop(columns=['Item_Identifier'])
ohe = OneHotEncoder(handle_unknown='ignore')
ohe.fit(cat_feats)
ohe_feature_names = ohe.get_feature_names_out(input_features=cat_feats.columns)

num_feats_train = X_train.select_dtypes(exclude=['object']).reset_index(drop=True)
cat_feats_train = X_train.select_dtypes(include=['object']).drop(columns=['Item_Identifier'])
X_train_cat_ohe = pd.DataFrame(ohe.transform(cat_feats_train).toarray(),columns=ohe_feature_names)
X_train_final=pd.concat([num_feats_train,X_train_cat_ohe],axis=1)
X_train_final.head()

X_train_final.shape

gb = GradientBoostingRegressor(random_state=SEED)
train_and_eval_model(model=gb, X_train=X_train_final, y_train=y_train)

hgb = HistGradientBoostingRegressor(random_state=SEED)
train_and_eval_model(model=hgb, X_train=X_train_final, y_train=y_train)

xgr = xgb.XGBRegressor(objective='reg:squarederror',random_state=SEED)
train_and_eval_model(model=xgr, X_train=X_train_final, y_train=y_train)

lgbr = LGBMRegressor(random_state=SEED)
train_and_eval_model(model=lgbr, X_train=X_train_final, y_train=y_train)

"""## Expt 4: Item Identifier-Feature hashed,rest categorical-one hot encoded"""

from sklearn.feature_extraction import FeatureHasher

hash_vector_size = 50
fh = FeatureHasher(n_features=hash_vector_size, input_type='string')
hashed_df = pd.DataFrame(fh.transform([[item_id] for item_id in X_train['Item_Identifier']]).toarray(), columns=['H'+str(i) for i in range(hash_vector_size)])
hashed_df.head()

cat_feats = X_train.select_dtypes(include=['object']).drop(columns=['Item_Identifier'])
ohe = OneHotEncoder(handle_unknown='ignore')
ohe.fit(cat_feats)
ohe_feature_names = ohe.get_feature_names_out(input_features=cat_feats.columns)

num_feats_train = X_train.select_dtypes(exclude=['object']).reset_index(drop=True)
cat_feats_train = X_train.select_dtypes(include=['object']).drop(columns=['Item_Identifier'])
X_train_cat_ohe = pd.DataFrame(ohe.transform(cat_feats_train).toarray(),columns=ohe_feature_names)
X_train_final=pd.concat([num_feats_train,hashed_df,X_train_cat_ohe],axis=1)
X_train_final.head()

gb = GradientBoostingRegressor(random_state=SEED)
train_and_eval_model(model=gb, X_train=X_train_final, y_train=y_train)

xgr = xgb.XGBRegressor(objective='reg:squarederror',random_state=SEED)
train_and_eval_model(model=xgr, X_train=X_train_final, y_train=y_train)

X_test.shape

hashed_test_df = pd.DataFrame(fh.transform([[item_id] for item_id in X_train['Item_Identifier']]).toarray(), columns=['H'+str(i) for i in range(hash_vector_size)])
num_feats_test = X_test.select_dtypes(exclude=['object']).reset_index(drop=True)
cat_feats_test = X_test.select_dtypes(include=['object']).drop(columns=['Item_Identifier'])
X_test_cat_ohe = pd.DataFrame(ohe.transform(cat_feats_test).toarray(),columns=ohe_feature_names)
X_test_final=pd.concat([num_feats_test,hashed_test_df,X_test_cat_ohe],axis=1)
X_test_final.head()

X_test_final.shape

xgr = xgb.XGBRegressor(objective='reg:squarederror',random_state=SEED)
xgr.fit(X_train_final,y_train)

y_pred=xgr.predict(X_test_final)

from sklearn.metrics import r2_score,mean_squared_error

print('R2 Score:',r2_score(y_test,y_pred))
print('RMSE Score:',mean_squared_error(y_test,y_pred,squared=False))

from xgboost import plot_importance
fig,ax = plt.subplots(1,1,figsize=(20,10))
plot_importance(xgr,ax=ax)

